# üö≤ Cycle World - An√°lisis de Datos y Dashboard Interactivo con Snowflake & Streamlit üöÄ

## üìù Resumen del Proyecto

Este proyecto representa el ejercicio final de un proceso de selecci√≥n y formaci√≥n enfocado en **Snowflake**, la plataforma de datos en la nube. El objetivo principal fue simular un escenario real para la empresa ficticia "Cycle World" en Londres, que inicia su transformaci√≥n digital y necesita analizar datos de uso de bicicletas que originalmente resid√≠an en archivos dispersos (.csv, .xlsx).

El desaf√≠o consisti√≥ en realizar un **proceso completo de ETL/ELT y an√°lisis de datos utilizando Snowflake como plataforma central**, culminando en la creaci√≥n de un **dashboard interactivo en Streamlit** que responde a requerimientos espec√≠ficos del negocio y preguntas anal√≠ticas clave. Se puso √©nfasis en seguir buenas pr√°cticas, optimizar (considerando costos y rendimiento) y documentar cada paso del proceso.

**Tecnolog√≠as Principales:** Snowflake (SQL), Streamlit, Python (para carga inicial y app), Git/GitHub.

---

## üéØ Descripci√≥n del Ejercicio

Cycle World necesitaba comenzar a explotar la informaci√≥n de sus operaciones de bicicletas compartidas en Londres. Los datos disponibles proven√≠an de archivos separados:

1.  **Journeys (`Journeys.csv`):** Archivo plano (delimitado por `;`) con detalles de cada viaje realizado. Contiene informaci√≥n como:
    * `Journey Duration`: Duraci√≥n en segundos.
    * `Journey ID`: Identificador (¬°Ojo! Se descubri√≥ que no es √∫nico por viaje).
    * `End Date/Month/Year/Hour/Minute`: Componentes de la fecha/hora de fin.
    * `End Station ID`: ID de la estaci√≥n de fin.
    * `Start Date/Month/Year/Hour/Minute`: Componentes de la fecha/hora de inicio (A√±o '11' necesita correcci√≥n).
    * `Start Station ID`: ID de la estaci√≥n de inicio.
    * `Bike ID`: ID de la bicicleta usada.

2.  **Weather (`Weather.csv`):** Archivo plano (delimitado por `,`) con datos clim√°ticos horarios. Incluye:
    * `datetime`: Fecha y hora de la medici√≥n.
    * `season`: C√≥digo de estaci√≥n del a√±o.
    * `holiday`: Indicador de d√≠a festivo.
    * `workingday`: Indicador de d√≠a laboral.
    * `weather`: C√≥digo num√©rico del estado del tiempo (1: Despejado, 2: Neblina, 3: Lluvia/Nieve Ligera, 4: Lluvia/Nieve Intensa/Tormenta).
    * `temp`: Temperatura (¬∞C).
    * `atemp`: Sensaci√≥n t√©rmica (¬∞C).
    * `humidity`: Humedad (%).
    * `windspeed`: Velocidad del viento.
    * `casual`, `registered`, `count`: Usuarios por tipo y totales (¬°Info interesante adicional!).

3.  **Stations & Bikes (`Stations_-_Bikes.xlsx`):** Archivo Excel con dos hojas:
    * **Hoja 'stations'**: Informaci√≥n de las estaciones.
        * `Station ID`: Identificador √∫nico de la estaci√≥n.
        * `Capacity`: Capacidad m√°xima de bicicletas.
        * `Latitude`, `Longitude`: Coordenadas geogr√°ficas.
        * `Station Name`: Nombre (ej: "River Street , Clerkenwell", ¬°incluye localidad!).
    * **Hoja 'bikes'**: Informaci√≥n de las bicicletas.
        * `Bike ID`: Identificador √∫nico de la bicicleta.
        * `Bike model`: Modelo (ej: CLASSIC, PBSC_EBIKE).
        * `Bike color`: Color (ej: Blue, Red, Black).

---

## üõ†Ô∏è Proceso de Desarrollo Detallado

Se sigui√≥ un enfoque estructurado en fases, utilizando Snowflake como motor principal:

### Fase 1: Configuraci√≥n y Carga de Datos RAW üì•

1.  **Entorno Snowflake:** Se cre√≥ una base de datos (`CYCLE_WORLD_DB`), esquemas separados (`RAW`, `PROCESSED`, `ANALYTICS`) para organizaci√≥n y un Warehouse virtual (`CYCLE_WORLD_WH`) para el c√≥mputo.
2.  **Staging:** Se crearon Stages internos nombrados en el schema `RAW` para cada archivo fuente (`JOURNEYS_CSV_STAGE`, `WEATHER_CSV_STAGE`, `BIKES_XLSX_STAGE`, `STATIONS_XLSX_STAGE`). Los archivos fueron subidos a estos stages.
3.  **Inspecci√≥n desde Stage:** Se utiliz√≥ `SELECT $N... FROM @stage` e `INFER_SCHEMA` para analizar la estructura, delimitadores (`,` para Weather, `;` para Journeys) y contenido directamente desde los stages antes de cargar. Se detect√≥ el formato de a√±o '11' en Journeys y la estructura con comas y comillas en Station Name (Excel).
4.  **File Formats:** Se crearon `FILE_FORMAT` espec√≠ficos (`FF_CSV_COMMA`, `FF_CSV_SEMICOLON`) definiendo delimitadores, manejo de encabezados (`SKIP_HEADER=1`), valores nulos (`EMPTY_FIELD_AS_NULL=TRUE`) y, crucialmente, el manejo de campos opcionalmente encerrados por comillas (`FIELD_OPTIONALLY_ENCLOSED_BY = '"'`) para interpretar correctamente los nombres de estaci√≥n.
5.  **Manejo de Excel:** Dada la dificultad inicial con Snowpark y la limitaci√≥n de tiempo, se opt√≥ por la soluci√≥n pragm√°tica de **convertir manualmente las hojas 'stations' y 'bikes' del `.xlsx` a archivos `.csv` separados** (`Stations_from_excel.csv`, `Bikes_from_excel.csv`). Estos CSVs se subieron a los stages.
6.  **Creaci√≥n de Tablas RAW:** Se crearon tablas en el schema `RAW` (`RAW_JOURNEYS`, `RAW_WEATHER`, `RAW_STATIONS`, `RAW_BIKES`) con **todas las columnas como `VARCHAR`** para una carga inicial robusta y flexible. Se a√±adieron columnas de metadatos (`_FILE_NAME`, `_FILE_ROW_NUMBER`, `_LOAD_TIMESTAMP`).
7.  **Carga (`COPY INTO`):** Se utiliz√≥ `COPY INTO RAW_TABLE FROM @stage FILE_FORMAT = ... ON_ERROR = 'CONTINUE'` para cargar los datos desde los archivos CSV (incluyendo los convertidos del Excel) a sus respectivas tablas RAW.

### Fase 2: Transformaci√≥n y Modelado - Schema PROCESSED ‚ú®

El objetivo fue limpiar los datos RAW y crear tablas estructuradas con tipos de datos correctos y relaciones impl√≠citas (modelo dimensional b√°sico: Dimensiones y Hechos).

1.  **`PROCESSED.DIM_STATIONS` (Dimensi√≥n Estaciones):**
    * Se leyeron datos de `RAW_STATIONS`.
    * Se convirtieron IDs y capacidad a `NUMBER`, coordenadas a `FLOAT` usando `TRY_CAST`.
    * Se limpi√≥ `STATION_NAME` eliminando comillas (`TRIM(..., '"')`) y separando el nombre principal de la localidad usando `SPLIT_PART(..., ',', 1)`.
    * Se deriv√≥ `SECTOR` usando la localidad (la parte despu√©s de la coma) con `SPLIT_PART(..., ',', 2)` y `TRIM()`.
    * Se verific√≥ la unicidad de `STATION_ID` y se estableci√≥ como **Clave Primaria Natural**.

2.  **`PROCESSED.DIM_BIKES` (Dimensi√≥n Bicicletas):**
    * Se leyeron datos de `RAW_BIKES`.
    * Se convirti√≥ `BIKE_ID` a `NUMBER` (`TRY_CAST`).
    * Se limpiaron (`TRIM`) y estandarizaron (`UPPER`) `BIKE_MODEL` y `BIKE_COLOR`.
    * Se verific√≥ la unicidad de `BIKE_ID` y se estableci√≥ como **Clave Primaria Natural**.

3.  **`PROCESSED.FACT_JOURNEYS` (Hechos Viajes):**
    * Se leyeron datos de `RAW_JOURNEYS`.
    * Se convirtieron IDs y duraci√≥n a `NUMBER` (`TRY_CAST`). Se calcul√≥ `JOURNEY_DURATION_MINUTES`.
    * **Manejo de `JOURNEY_ID`:** Se descubri√≥ que `JOURNEY_ID` **NO era √∫nico por viaje**, y conten√≠a viajes distintos (incluso con bicis distintas) bajo el mismo ID. Para garantizar una clave √∫nica por *fila/evento de viaje*, se cre√≥ una **Clave Sustituta (`TRIP_SK`)** usando una `SEQUENCE` de Snowflake (`SEQ_TRIP_SK.NEXTVAL`). El `JOURNEY_ID` original se conserv√≥ como referencia.
    * Se combinaron los componentes de fecha/hora de inicio y fin, corrigiendo el a√±o '11' a '2011' (`CASE WHEN...`), asegurando formato de 2 d√≠gitos (`LPAD`), y usando `TRY_TO_TIMESTAMP_NTZ` para crear las columnas `START_TIMESTAMP` y `END_TIMESTAMP`.
    * Se a√±adi√≥ `TRIP_SK` como **Clave Primaria**.

4.  **`PROCESSED.FACT_WEATHER` (Hechos/Dimensi√≥n Clima):**
    * Se leyeron datos de `RAW.RAW_WEATHER`.
    * Se convirti√≥ `DATETIME` a `TIMESTAMP_NTZ`. Se extrajeron `WEATHER_DATE` y `WEATHER_HOUR`.
    * Se convirtieron c√≥digos (`SEASON`, `WEATHER`) a `NUMBER`, indicadores (`HOLIDAY`, `WORKINGDAY`) a `BOOLEAN`, y mediciones (`TEMP`, `ATEMP`, etc.) a `FLOAT` o `NUMBER`.
    * Se a√±adieron columnas descriptivas (`SEASON_DESC`, `WEATHER_DESC`) usando `CASE`.
    * Se renombraron columnas para mayor claridad (ej: `TEMP_CELSIUS`).
    * Se a√±adi√≥ una **Clave Sustituta (`WEATHER_SK`)** usando una secuencia (`SEQ_WEATHER_SK.NEXTVAL`) para asegurar unicidad por fila y facilitar posibles uniones, estableci√©ndola como **Clave Primaria**.

### Fase 3: An√°lisis y Vistas - Schema ANALYTICS üìä

Se crearon Vistas (Views) en el schema `ANALYTICS` para encapsular la l√≥gica de cada requerimiento y pregunta, proporcionando una capa limpia para la herramienta de visualizaci√≥n (Streamlit).

* `JOURNEYS_TO_STATIONS_VIEW`: Para el Reporte #1 (Resumen Simple).
* `STATION_ACTIVITY`: Para el Reporte #2 (Actividad General).
* `STATION_ACTIVITY_MARYLEBONE`: Para el Reporte #3 (Actividad Marylebone).
* `TOP_TEN_STATIONS`: Para la Pregunta #4 (Top 10 Estaciones).
* `RAINY_PERCENTAGE`: Para la Pregunta #5 (% Viajes Lluviosos).
* `AVG_JOURNEY_SUNNY`: Para la Pregunta #6 (Duraci√≥n Promedio Sol).
* `BIKE_COLOR`: Para la Pregunta #7 (Uso Color Bici).

---

## üìà Resultados y Hallazgos Clave

Consultando las Vistas en `ANALYTICS`, se obtuvieron respuestas a los requerimientos:

* Se generaron los reportes tabulares solicitados.
* Se identificaron las 10 estaciones m√°s concurridas (ver dashboard).
* Se calcul√≥ el porcentaje de viajes que ocurrieron en d√≠as lluviosos (ver dashboard).
* Se calcul√≥ la duraci√≥n promedio de viaje en d√≠as despejados (ver dashboard).
* Se identific√≥ el color de bicicleta m√°s y menos utilizado (ver dashboard).
* **Hallazgo Cr√≠tico - Pregunta #8:** El an√°lisis para determinar si alguna estaci√≥n se qued√≥ sin bicicletas se vio **imposibilitado por inconsistencias graves en los datos fuente** de `Journeys.csv`. Se detectaron registros de salidas que superan masivamente la capacidad f√≠sica de las estaciones (ej: Estaci√≥n 14 registr√≥ 181 bicicletas distintas saliendo en una sola hora, teniendo capacidad 48). **Conclusi√≥n:** No se puede responder fiablemente a esta pregunta; se recomienda una revisi√≥n profunda del sistema de origen que genera estos datos.

---

## üñ•Ô∏è Aplicaci√≥n Streamlit

Se desarroll√≥ una aplicaci√≥n web interactiva utilizando Streamlit para visualizar los resultados:

* **Conexi√≥n:** La aplicaci√≥n (versi√≥n para despliegue externo) se conecta a la base de datos `CYCLE_WORLD_DB` en Snowflake usando `st.connection("snowflake")` y credenciales seguras gestionadas por Streamlit Secrets.
* **Navegaci√≥n:** Una barra lateral permite navegar entre dos p√°ginas principales:
    * **P√°gina 1: Reportes Tabulares üìÑ:** Muestra los resultados de los Reportes #1, #2 y #3 en formato de tabla (`st.dataframe`). Incluye un campo de entrada num√©rica (`st.number_input`) para que el usuario defina cu√°ntas filas desea visualizar en cada tabla.
    * **P√°gina 2: Dashboard Anal√≠tico üìä:** Presenta visualizaciones para las preguntas anal√≠ticas:
        * Un `st.metric` destacando la estaci√≥n m√°s concurrida y un gr√°fico de barras (`st.bar_chart`) del Top 10.
        * Un `st.metric` destacando el color de bicicleta m√°s usado y un gr√°fico de barras (`st.bar_chart`) mostrando la distribuci√≥n por color.
        * Dos `st.metric` lado a lado mostrando el % de viajes lluviosos y la duraci√≥n promedio en d√≠as soleados.
        * Una nota aclaratoria sobre la imposibilidad de responder la Pregunta #8.

---

## üíª Tecnolog√≠as Utilizadas

* **Snowflake:** Plataforma de datos en la nube (Base de Datos, C√≥mputo, Stages, SQL, Secuencias, Vistas).
* **SQL:** Lenguaje principal para DDL, DML, Carga (COPY INTO), Transformaci√≥n (CTAS) y Consultas.
* **Streamlit:** Framework de Python para construir la aplicaci√≥n web interactiva y el dashboard.
* **Python:** Utilizado para escribir la aplicaci√≥n Streamlit (con `pandas` para manejo de datos interno).
* **Git / GitHub:** Para control de versiones y alojamiento del c√≥digo.
* **(Intento fallido) Snowpark:** Se intent√≥ usar Snowpark para la carga de Excel, pero se encontraron dificultades con el entorno de paquetes en el tiempo disponible.

---

## üöÄ Ejecuci√≥n / Visualizaci√≥n

*(Aqu√≠ podr√≠as poner el enlace a tu aplicaci√≥n desplegada en Streamlit Community Cloud si ya lo tienes)*

Ejemplo:
`Puedes ver la aplicaci√≥n desplegada aqu√≠: [Enlace a tu App Streamlit]`

*(O instrucciones b√°sicas si fuera necesario correrla localmente, aunque para la entrega el link es mejor)*

---

## ü§î Conclusiones y Pr√≥ximos Pasos

* Se complet√≥ exitosamente un ciclo ELT (Extract-Load-Transform) y de an√°lisis utilizando Snowflake, demostrando la capacidad de la plataforma para ingestar, procesar y consultar datos de diversas fuentes.
* Se construy√≥ un dashboard funcional en Streamlit que responde a la mayor√≠a de los requerimientos del negocio.
* **El hallazgo m√°s significativo es la pobre calidad de los datos de viajes (`Journeys.csv`)**, lo cual impide an√°lisis de inventario fiables y deber√≠a ser el punto prioritario a resolver en un escenario real.
* **Mejoras Futuras:**
    * Investigar y corregir los problemas de datos en origen.
    * Implementar una simulaci√≥n de inventario m√°s robusta si los datos se corrigen.
    * Utilizar librer√≠as gr√°ficas m√°s avanzadas (Altair, Plotly) en Streamlit para mayor personalizaci√≥n visual.
    * Profundizar el an√°lisis (ej: rutas m√°s comunes, an√°lisis por hora del d√≠a m√°s detallado, etc.).
    * Optimizar consultas y Vistas si el volumen de datos creciera significativamente.

---

¬°Espero que este README sea completo y refleje todo el excelente trabajo que realizaste! ¬°Mucha suerte con tu presentaci√≥n y el video! ü§û
